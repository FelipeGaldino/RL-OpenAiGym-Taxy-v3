{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAIGym_Taxy-V3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYuuUWhzr0QC",
        "colab_type": "text"
      },
      "source": [
        "# OpenAI \n",
        "\n",
        "A OpenAI é uma organização de pesquisa em inteligência artificial que visa promover e desenvolver inteligência artificial de maneiras que beneficiem a humanidade como um todo. \n",
        "\n",
        "[Site OpenAi](https://openai.com/)\n",
        "\n",
        "[GitHub -OpenAI](https://github.com/openai)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol86ahQFskCa",
        "colab_type": "text"
      },
      "source": [
        "# OpenAI Gym\n",
        "\n",
        "O OpenAi Gym é um kit de ferramentas para desenvolver e comparar algoritmos de aprendizado por reforço. Neste exemplo estaremos utilizando o ambiente do Taxy-V3.\n",
        "\n",
        "[Gym Taxy-V3](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_HevfsetXii",
        "colab_type": "text"
      },
      "source": [
        "# Taxy-V3\n",
        "\n",
        "AMBIENTE\n",
        "\n",
        "![Taxy-V3](https://user-images.githubusercontent.com/45602322/80442708-880d2680-88db-11ea-84de-aae1e7adba4c.gif)\n",
        "\n",
        "\n",
        "O ambiente Taxi-v3 é exibido acima como uma simulação baseada em texto. \n",
        "\n",
        "* São 500 estados possiveis.\n",
        "\n",
        "* São 6 Ações possiveis.\n",
        "\n",
        "* Agente Taxi \n",
        "\n",
        "    * Amarelo sem passageiro.\n",
        "    * Verde com passageiro.\n",
        "\n",
        "* Evite os muros `(\"|\")`.\n",
        "\n",
        "* Cruze caminhos seguros `(\":\")`.\n",
        "\n",
        "\n",
        "Na simulação acima, o Taxi (retângulo) inicia em um local aleatório e, em seguida, pega um passageiro em B, passando de amarelo para verde. O Taxi então segue para o local de desembarque G, completando um episódio. \n",
        "\n",
        "Um episódio é definido como uma sequência de estados, ações e recompensas que terminam com um estado terminal. \n",
        "\n",
        "Neste caso, um episódio é concluído quando o taxista pega um passageiro com sucesso e o deixa no local correto. \n",
        "\n",
        "O táxi deseja concluir cada episódio da maneira mais eficiente possível, cruzando caminhos seguros `(\":\")` e evitando muros `(\"|\")`. \n",
        "\n",
        "Como o agente é motivado por recompensa, devemos decidir com cuidado as recompensas e penalidades e suas magnitudes de acordo. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNpKt_6Mp0my",
        "colab_type": "text"
      },
      "source": [
        "# Estados e Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLvQ_zU_rz1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym                                      # Importa o modulo com os ambientes OpenAiGym\n",
        "env = gym.make('Taxi-v3')                       # Funcao carrega o ambiente na variavel env\n",
        "\n",
        "Estados_Possiveis = env.observation_space.n     # Atribui a quantidade de estados possiveis\n",
        "Acoes_Possiveis   = env.action_space.n          # Atribui a quantidade de Ações possiveis\n",
        "\n",
        "Estado = env.reset()                            # Redefine o ambiente e retorna um estado inicial aleatório.\n",
        "\n",
        "env.render()                                    # Renderiza o ambiente.\n",
        "\n",
        "print(f\" O Estado Aleatorio é o {Estado}!\")  \n",
        "print(f\"\\n Temos {Estados_Possiveis} Estados possiveis no Ambiente!\" ) \n",
        "print(f\"\\n Temos {Acoes_Possiveis} Ações possiveis no Ambiente!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vdszGwy1Rft",
        "colab_type": "text"
      },
      "source": [
        "# Lista de Ações\n",
        "\n",
        "* 0 - Mover para o sul    - (South) \n",
        "* 1 - Mover para o norte  - (North) \n",
        "* 2 - Mover para o leste  - (East)\n",
        "* 3 - Mover para o oeste  - (West)\n",
        "* 4 - Pegar               - (Pickup)\n",
        "* 5 - Deixar              - (Dropoff)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvM5pOTzrzs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mostrar todas as Ações possiveis\n",
        "import gym                                      \n",
        "env = gym.make('Taxi-v3') \n",
        "\n",
        "Acao = 0\n",
        "env.reset() \n",
        "while Acao <= 5:\n",
        "  env.step(Acao) # (Ação) Avança o ambiente em um timestep.\n",
        "  env.render()\n",
        "  print(f\"Esta ação corresponde ao numero {Acao}.\")\n",
        "  Acao = Acao + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOW4o4-A5HSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gera uma ação aleatoria das 6 possiveis.\n",
        "import gym                                      \n",
        "env = gym.make('Taxi-v3') \n",
        "\n",
        "env.reset() \n",
        "\n",
        "Acao = env.action_space.sample()  \n",
        "\n",
        "env.step(Acao)\n",
        "env.render()\n",
        "\n",
        "print(f\" A Ação Aleatoria é {Acao}\")   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QLOw-VC4ORz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Escolha uma ação das 6 possiveis.\n",
        "import gym                                      \n",
        "env = gym.make('Taxi-v3') \n",
        "env.reset() \n",
        "\n",
        "Acao = int(input(\" Digite um numero de 0 a 5 \")) \n",
        "\n",
        "env.step(Acao)\n",
        "env.render()\n",
        "\n",
        "print(f\" A Ação Escolhida foi {Acao}\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kMB6WAHFlUz",
        "colab_type": "text"
      },
      "source": [
        "# Retorno do ambiente\n",
        "\n",
        "* observation - É um objeto específico do ambiente que representa sua observação do ambiente.\n",
        "\n",
        "* reward - Quantidade de recompensa / pontuação alcançada pela ação anterior.\n",
        "\n",
        "     * (+20) O agente deve receber uma alta recompensa positiva por uma viagem (Drop-off) bem-sucedida.\n",
        "\n",
        "     * (-10) O agente deve ser penalizado se tentar deixar um passageiro em locais errados.\n",
        "\n",
        "     * (-1)  O agente deve receber uma pequena penalidade por não chegar ao destino após cada intervalo de tempo. Preferimos que nosso agente chegue mais tarde, em vez de fazer movimentos errados tentando chegar ao destino o mais rápido possível\n",
        "\n",
        "* done - Indica se é hora de redefinir o ambiente novamente, ou seja, o agente alcançou a meta.\n",
        "\n",
        "*  info - informações de diagnóstico, como desempenho e latência, são úteis para fins de depuração.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TvLk3fJz1Tu",
        "colab_type": "text"
      },
      "source": [
        "# Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUbVDEduJGg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym                                      \n",
        "env = gym.make('Taxi-v3') \n",
        "\n",
        "env.reset()                                                                   \n",
        "observation, reward, done, info = env.step(env.action_space.sample())  # env.step obtem uma açao aleatoria\n",
        "   \n",
        "env.render() \n",
        "\n",
        "print(f\"O Estado aleatorio observado foi : {observation}\")\n",
        "print(f\"A Recompensa da Ação executada neste estado aleatorio foi : {reward}\")\n",
        "print(f\"O agente alcançou a meta? {done}\")\n",
        "print(f\"Informações? {info}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc0tKnV48UF_",
        "colab_type": "text"
      },
      "source": [
        "# Loop de Tarefas Aleatorio\n",
        "\n",
        "- Chamaremos env.step( ) repetidamente junto com o env.action_space.sample( ) como argumento para obter uma ação aleatória das seis ações permitidas.\n",
        "- Em seguida, o agente avançara uma etapa.\n",
        "- Definimos a condição final do loop no ponto em que o táxi recebe uma recompensa de 20.\n",
        "\n",
        "Missão\n",
        "\n",
        "*  Deixe o passageiro no local certo.\n",
        "*  Economize tempo do passageiro, minimizando timesteps.\n",
        "*  Garanta a segurança dos passageiros e siga as regras de trânsito."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkXSxExk8WdT",
        "colab_type": "text"
      },
      "source": [
        "# Loop Aleatorio\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb6PfJq5rzl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym                                   \n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "state = env.reset() \n",
        "count = 0          \n",
        "reward = 0         \n",
        "\n",
        "penalties = 0 \n",
        "reward = 0\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "while reward !=20:   \n",
        "    action = env.action_space.sample()                                                             # Se a recompensa for diferente de 20 ou seja se for 10 ou -1 faça\n",
        "    observation, reward, done, info = env.step(action)         # env.step obtem uma açao aleatoria\n",
        "    count += 1\n",
        "    \n",
        "    if reward == -10:\n",
        "      penalties += 1\n",
        "\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'episode': '0',\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(f\" Tivemos {penalties} penalidades, a ultima recompensa foi de : {reward}\") # Imprime quantas etapas aleatorias o agente executou para finalizar a tarefa\n",
        "print(f\"Percorremos {count} etapas para conseguir resolver o jogo! \") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LNRLPSiuKUM",
        "colab_type": "text"
      },
      "source": [
        "## Visualiza o Loop aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CVfJo9Ij6aJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função que imprime os resultados na tela\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Episodio: {frame['episode']}\")\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"Estado: {frame['state']}\")\n",
        "        print(f\"Ação: {frame['action']}\")\n",
        "        print(f\"Recompensa: {frame['reward']}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "print_frames(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFdgZTJ6C6JD",
        "colab_type": "text"
      },
      "source": [
        "# Q-Learning\n",
        "\n",
        "O Q-learning é um método de aprendizado fora da política, isso significa que ele não segue nenhuma política para encontrar a próxima ação, mas escolhe a ação com base em um estilo ganancioso. \n",
        "\n",
        "Conforme o agente é criado ele alimenta uma tabela chamada de Q-table e nela contem os valores das recompensas para cada estado do ambiente.\n",
        "\n",
        "![Q-Table](https://user-images.githubusercontent.com/45602322/80503075-62196d80-893f-11ea-922d-4b376f2d876a.png)\n",
        "\n",
        "\n",
        "À medida que o agente escolhe ações para cada estado, dependendo da recompensa que recebe pela ação, o valor Q será atualizado usando a equação de Bellman:\n",
        "\n",
        "![Equação de Bellman](https://user-images.githubusercontent.com/45602322/80502482-b5d78700-893e-11ea-8a70-c62748d39c45.png)\n",
        "\n",
        "* α (Alpha) é a taxa de aprendizado, até que ponto os valores Q são atualizados por iteração.\n",
        "\n",
        "* γ (Gamma) é o fator de desconto que determina quanta importância é dada às recompensas futuras.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsYWjqYQVaMp",
        "colab_type": "text"
      },
      "source": [
        "# Quebrando o algoritmo de Q-learning, temos as seguintes etapas:\n",
        "\n",
        "* 1 Inicialize a tabela Q com zeros e valores Q em constantes arbitrárias.\n",
        "* 2 Explorar ações: para cada mudança de estado, selecione qualquer ação (a) entre todas as ações possíveis para o estado atual (S).\n",
        "\n",
        "* 3 Vá para o próximo estado (S ') como resultado da ação (a).\n",
        "* 4 Para todas as ações possíveis do estado (S '), selecione aquela com o valor Q mais alto.\n",
        "* 5 Atualize os valores da tabela Q usando a equação\n",
        "* 6 Defina o próximo estado como o estado atual.\n",
        "* 7 Se o estado do terminal for alcançado, finalize e repita o processo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrZaCb5hXmLs",
        "colab_type": "text"
      },
      "source": [
        "# Q-Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLp1A3agM4Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inicializa Q Table com 0 e imprime\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "Q_Table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "print(Q_Table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKMMGWYmVnW7",
        "colab_type": "code",
        "outputId": "109af560-1e7d-4262-c5de-6f8cfb9fc8a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Q Table vindo do Ambiente \n",
        "# Voce pode escolher qualquer estado dentre os 500 para ver\n",
        "env.P[499]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 499, -1, False)],\n",
              " 1: [(1.0, 399, -1, False)],\n",
              " 2: [(1.0, 499, -1, False)],\n",
              " 3: [(1.0, 479, -1, False)],\n",
              " 4: [(1.0, 499, -10, False)],\n",
              " 5: [(1.0, 499, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qU1dH2oVFsA",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparametros\n",
        "\n",
        "* Alpha é a taxa de aprendizado, até que ponto os valores Q são atualizados por iteração.\n",
        "\n",
        "* Gamma é o fator de desconto que determina quanta importância é dada às recompensas futuras.\n",
        "\n",
        "* Epsilon é a medida da taxa de exploração no ambiente a medida que desenvolvemos nossa estratégia, temos menos necessidade de explorar o ambiente, portanto, à medida que os testes aumentam, o epsilon deve diminuir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h3ptZNxNNK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym    \n",
        "import numpy as np               \n",
        "env = gym.make('Taxi-v3') \n",
        "\n",
        "state = env.reset() \n",
        "reward = 0\n",
        "count = 0          \n",
        "        \n",
        "gamma = 0.9\n",
        "alpha = 0.9\n",
        "\n",
        "Q_Table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "while reward !=20: \n",
        "\n",
        "    action = np.argmax(Q_Table[state])\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)                                                        \n",
        "\n",
        "    Q_Table[state, action] = Q_Table[state, action] + alpha * (reward + gamma * np.max(Q_Table[next_state]) - Q_Table[state, action])\n",
        "    \n",
        "    state = next_state\n",
        "\n",
        "    count += 1\n",
        "\n",
        "env.render() \n",
        "                                        \n",
        "print(f\"Percorremos {count} etapas para conseguir resolver o jogo! e tivemos a recompensa{reward} \") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw9w69OkuWIv",
        "colab_type": "text"
      },
      "source": [
        "# Treinando com a Q-Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBT1uNA9a2kN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Treinando o Agente\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# Plota metricas\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "episodios = 100001\n",
        "\n",
        "for i in range(1, episodios):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explora espço de ação\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Explorar valores aprendidos\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\" Passou o Episodio: {i}\")\n",
        "\n",
        "print(\" O Treino Finalizou! \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp-o19-eufoC",
        "colab_type": "text"
      },
      "source": [
        "# Testando o Agente treinado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiMnzpnFa2gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testando o Agente\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "import gym    \n",
        "import numpy as np       \n",
        "import time        \n",
        "env = gym.make('Taxi-v3') \n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "frames = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "        \n",
        "        # Coloca os quadros para a animação\n",
        "        frames.append({\n",
        "            'frame': env.render(mode='ansi'),\n",
        "            'episode': ep, \n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward\n",
        "            }\n",
        "        )\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Total de Episodios : {episodes} \")\n",
        "print(f\"Total de Epocas percorridas :  {total_epochs} \")\n",
        "print(f\"Total de Penalidades por episodio: {total_penalties}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-uq5SxlupP_",
        "colab_type": "text"
      },
      "source": [
        "## Visualiza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMT5m5PEa2eR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função que imprime os resultados na tela\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Episodio: {frame['episode']}\")\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"Estado: {frame['state']}\")\n",
        "        print(f\"Ação: {frame['action']}\")\n",
        "        print(f\"Recompensa: {frame['reward']}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "print_frames(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-kG37Kyt8-R",
        "colab_type": "text"
      },
      "source": [
        "# Fim\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}